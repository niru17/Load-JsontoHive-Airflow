# Load JSON Data from GCS to Hive on Dataproc

![DAG Flow](DAG_flow.png)

## ğŸ“Œ Objective

This project implements an Apache Airflow DAG that performs the following:

- Downloads a JSON file (`Employee.json`) from a Google Cloud Storage (GCS) bucket.
- Submits a PySpark job to a Dataproc cluster that:
  - Creates a Hive database and table (if not already present).
  - Reads the JSON file, infers schema, and appends data into a Hive table (`EMP_DB.employee`) stored as Parquet.

---

## ğŸ§° Tech Stack

- **Airflow** (Python DAG)
- **Google Cloud Platform**
  - Cloud Storage
  - Dataproc
- **Apache Spark** (via PySpark)
- **Hive** (on Dataproc)
- **Python**

---

## ğŸ“ File Structure

```bash
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ Load_json_to_hive.py         # Airflow DAG definition
â”‚   â”œâ”€â”€ spark_job/
â”‚   â”‚   â””â”€â”€ spark_job.py             # PySpark script executed on Dataproc
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ Employee.json            # Input JSON data in GCS
â”‚   â””â”€â”€ DAG_flow.png                 # DAG architecture image

